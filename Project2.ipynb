{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics.functional import retrieval_normalized_dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_count = os.cpu_count()\n",
    "device = torch.device(\"cpu\")\n",
    "time_size = 10\n",
    "embed_dim = 128\n",
    "embed_max = 256\n",
    "label_num = 527\n",
    "top_label_num = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_tensor(data_names, top_label_num=10, top_label_idx=None):\n",
    "    df = pd.DataFrame([])\n",
    "    for data_name in data_names:\n",
    "        add_df = pd.read_parquet(f'{data_name}.parquet')\n",
    "        add_df = add_df[~add_df.isnull()]\n",
    "        print(f\"{data_name}_df shape: \",add_df.shape)\n",
    "        add_df = add_df[add_df['audio_embedding'].apply(lambda x: len(x)) == time_size]\n",
    "        df = pd.concat([df,add_df],axis=0)\n",
    "        del add_df\n",
    "\n",
    "    def label_converter(x):\n",
    "        output = np.zeros(label_num,dtype=int)\n",
    "        for label in x:\n",
    "            output[label] = 1\n",
    "        return output\n",
    "\n",
    "    label_df = pd.DataFrame(np.vstack(df['labels'].apply(lambda x: label_converter(x))).reshape(-1,label_num))\n",
    "    top_label_idx = list(label_df.sum(axis=0).nlargest(10).index)\n",
    "    label_df = label_df[top_label_idx]\n",
    "    label_df = label_df[label_df.sum(axis=1)>0]\n",
    "    label_tensor = torch.Tensor(label_df.to_numpy())\n",
    "    print(f\"Total label shape: \",label_tensor.size())\n",
    "\n",
    "    df['label'] = df['label'].apply(lambda x: x[top_label_idx])\n",
    "    df = df[df['label'].apply(lambda x: sum(x))>0]\n",
    "\n",
    "    embeddings = np.vstack(df['audio_embedding'].apply(lambda x: np.vstack(x))).reshape(-1,time_size,embed_dim)\n",
    "    embedding_tensor = torch.Tensor(embeddings)\n",
    "    print(f\"Total embedding shape: \", embedding_tensor.size())\n",
    "    \n",
    "    assert embedding_tensor.shape[0] == label_tensor.shape[0], \"Feature and label dim does not coincide!\"\n",
    "    \n",
    "    return embedding_tensor/embed_max, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bal_train_df shape:  (21782, 5)\n",
      "eval_df shape:  (19976, 5)\n",
      "Total label shape:  torch.Size([22582, 10])\n",
      "Total embedding shape:  torch.Size([22582, 10, 128])\n",
      "Train data Size: 18064 , Test data Size : 4518\n"
     ]
    }
   ],
   "source": [
    "embedding_all, label_all = df_to_tensor(['bal_train','eval'], top_label_num)\n",
    "\n",
    "permute_idx = np.random.permutation(np.arange(embedding_all.shape[0]))\n",
    "train_idx = permute_idx[:(permute_idx.shape[0]//5)*4]\n",
    "test_idx = permute_idx[(permute_idx.shape[0]//5)*4:]\n",
    "print(f\"Train data Size: {len(train_idx)} , Test data Size : {len(test_idx)}\")\n",
    "\n",
    "train_embedding, train_label = embedding_all[train_idx], label_all[train_idx]\n",
    "test_embedding, test_label = embedding_all[test_idx], label_all[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TensorDataset(train_embedding, train_label)\n",
    "test_set = TensorDataset(test_embedding, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 16\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=bs, shuffle=True, num_workers=cpu_count)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=1, shuffle=False, num_workers=cpu_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoutubeAudioClassifier(nn.Module):\n",
    "    def __init__(self, time_size, embed_dim, fc_dim, output_dim):\n",
    "        super(YoutubeAudioClassifier, self).__init__()\n",
    "        self.time_size = time_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embed_dim1, self.embed_dim2 = self.split_embed_dim()\n",
    "        self.fc_dim = fc_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.intra_init_conv = nn.Sequential(\n",
    "            nn.Conv2d(self.time_size, self.time_size//2, 3, padding=1),    # 10*16*8 -> 5*16*8\n",
    "            nn.BatchNorm2d(self.time_size//2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.intra_stride = nn.Sequential(\n",
    "            nn.Conv2d(self.time_size//2, self.time_size//4, 2, stride=2),  # 5*16*8 -> 2*8*4\n",
    "            nn.BatchNorm2d(self.time_size//4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.intra_dim1_dil = nn.Sequential(\n",
    "            nn.Conv2d(self.time_size//2, self.time_size//2, 3, dilation=(2,1)),  # 5*16*8 -> 5*12*6\n",
    "            nn.BatchNorm2d(self.time_size//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.time_size//2, self.time_size//4, 3, dilation=(2,1)),  # 5*12*6 -> 2*8*4\n",
    "            nn.BatchNorm2d(self.time_size//4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.intra_dim2_dil = nn.Sequential(\n",
    "            nn.Conv2d(self.time_size//2, self.time_size//4, (2,3), dilation=(1,2), stride=(2,1)),  # 5*16*8 -> 2*8*4\n",
    "            nn.BatchNorm2d(self.time_size//4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.inter_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, (3,5), stride=(1,2)),  # 1*10*128 -> 4*8*62\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 8, (3,8), dilation=(1,3)),  # 4*8*62 -> 8*6*41\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.inter_max_conv = nn.Sequential(\n",
    "            nn.MaxPool2d((2,6), stride=(2,3), padding=(1,0)),  # 1*10*128 -> 1*6*41\n",
    "            nn.Conv2d(1, 8, 3, padding=1), # 1*6*41 -> 8*6*41\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.inter_conv2 = nn.Sequential(\n",
    "            nn.Conv2d(8, 4, (3,3), stride=(1,2)),  # 8*6*41 -> 4*4*20\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 2, (3,5), dilation=(1,3), padding=(1,0)),  # 4*4*20 -> 2*4*8\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.combine_norm = nn.Sequential(\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(self.fc_dim, self.output_dim)  # 64 -> 10\n",
    "\n",
    "\n",
    "    \n",
    "    # For balanced Width * Height split of input data for intra blocks    \n",
    "    def split_embed_dim(self):\n",
    "        for i in reversed(np.arange(np.ceil(np.sqrt(self.embed_dim))+1)):\n",
    "            if self.embed_dim % i == 0:\n",
    "                return self.embed_dim // int(i) , int(i)\n",
    "                break\n",
    "    \n",
    "    def forward(self, data):\n",
    "        intra_data = data.view(-1, self.time_size, self.embed_dim1, self.embed_dim2)\n",
    "        inter_data = data.view(-1, 1, self.time_size, self.embed_dim)\n",
    "        \n",
    "        intra_block1_out = self.intra_init_conv(intra_data)\n",
    "        intra_block2_out = self.intra_stride(intra_block1_out) + self.intra_dim1_dil(intra_block1_out) + self.intra_dim2_dil(intra_block1_out)\n",
    "        \n",
    "        inter_block1_out = self.inter_conv1(inter_data) + self.inter_max_conv(inter_data)\n",
    "        inter_block2_out = self.inter_conv2(inter_block1_out).transpose(-2,-1)\n",
    "        \n",
    "        cnn_out = self.combine_norm(intra_block2_out + inter_block2_out).view(-1,self.fc_dim)\n",
    "        fc_out = self.fc(cnn_out)\n",
    "        \n",
    "        return F.softmax(fc_out,dim=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoutubeAudioClassifierLight(pl.LightningModule):\n",
    "    def __init__(self, model, train_lossF, test_lossF):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.train_lossF = train_lossF\n",
    "        self.test_lossF = test_lossF\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        embedding, labels = batch\n",
    "        train_loss = self.train_lossF(self.model(embedding), labels.bool())\n",
    "        self.log(\"train_loss\", train_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return train_loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # this is the test loop\n",
    "        embedding, labels = batch\n",
    "        test_loss = self.test_lossF(self.model(embedding), labels.bool())\n",
    "        self.log(\"test_metric\", test_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return test_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_loss(loss_type, Precision_k=1):\n",
    "    if loss_type == 'msml':\n",
    "        if device == torch.device(\"mps\"):\n",
    "            def multilabel_soft_margin_loss(input, target, weight = None, size_average = None, reduce = None, reduction = \"mean\"):\n",
    "                # In MPS device, F.logsigmoid is not supported.\n",
    "                # It is well-known that softplus(beta = -1) is identical to logsigmoid.\n",
    "                loss = -(target * F.softplus(input, beta = -1) + (1 - target) * F.softplus(-input, beta = -1)) \n",
    "\n",
    "                if weight is not None:\n",
    "                    loss = loss * weight\n",
    "\n",
    "                class_dim = input.dim() - 1\n",
    "                C = input.size(class_dim)\n",
    "                loss = loss.sum(dim=class_dim) / C  # only return N loss values\n",
    "\n",
    "                if reduction == \"none\":\n",
    "                    ret = loss\n",
    "                elif reduction == \"mean\":\n",
    "                    ret = loss.mean()\n",
    "                elif reduction == \"sum\":\n",
    "                    ret = loss.sum()\n",
    "                else:\n",
    "                    ret = input\n",
    "                    raise ValueError(reduction + \" is not valid\")\n",
    "                return ret\n",
    "            return multilabel_soft_margin_loss\n",
    "        else:\n",
    "            return nn.MultiLabelSoftMarginLoss()\n",
    "    elif loss_type == 'ndcg':\n",
    "        return retrieval_normalized_dcg\n",
    "    elif loss_type == 'Precision@k':\n",
    "        def Precision_at_k(input, target):\n",
    "            topk_idx = torch.topk(input,Precision_k,dim=1).indices\n",
    "            return torch.stack([(target[i][topk_idx[i]]).float() for i in range(target.shape[0])]).mean()\n",
    "        return Precision_at_k\n",
    "    elif loss_type == 'CE':\n",
    "        def MultiCELoss(input, target):\n",
    "            loss = nn.CrossEntropyLoss()\n",
    "            nonzeros = target.nonzero()\n",
    "            idx, label = nonzeros[:,0], nonzeros[:,1]\n",
    "            norm_val = torch.bincount(idx)[idx]\n",
    "            norm_input = input[idx] / torch.bincount(idx)[idx].view(-1,1)\n",
    "            return loss(norm_input, label)\n",
    "        return MultiCELoss\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported Loss function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "YACLight = YoutubeAudioClassifierLight(YoutubeAudioClassifier(time_size, embed_dim, 64, top_label_num), define_loss(\"CE\"),define_loss(\"Precision@k\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "if device == torch.device(\"mps\"):\n",
    "    trainer = pl.Trainer(accelerator=\"mps\")\n",
    "elif device == torch.device(\"cuda\"):\n",
    "    trainer = pl.Trainer(accelerator=\"gpu\")\n",
    "else:\n",
    "    trainer = pl.Trainer(accelerator=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d88d26a1cf64c5d8057cefd6d74ec5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     test_loss_epoch       0.060174934566020966\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss_epoch': 0.060174934566020966}]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline precision (Before Training)\n",
    "trainer.test(model=YACLight, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type                   | Params\n",
      "-------------------------------------------------\n",
      "0 | model | YoutubeAudioClassifier | 3.0 K \n",
      "-------------------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20c945639e644cb942d9611748ef6e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model=YACLight, train_dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a0c7ca294974204866d0f0b1dba16a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     test_loss_epoch        0.8976417183876038\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss_epoch': 0.8976417183876038}]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model=YACLight, dataloaders=test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
